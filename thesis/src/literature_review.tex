\chapter{Literature Review}
\label{chap:literature_review}

This chapter briefly describes three projects,
Argus~\cite{argus_end_to_end_service_anomaly_detection_and_localization_from_an_isps_point_of_view},
NetNorad~\cite{netnorad},
and \gls*{cem}~\cite{crowdsourcing_service_level_network_event_monitoring},
that use end-to-end \gls*{qos} to identify and localize events, or in some cases
only faults, in computer networks.

These previous works share common approaches.
For instance, Argus and
NetNorad track the end-to-end \gls*{qos} evolution over time of different
end-hosts,
and the anomalies identified in these time series are interpreted as events.
Anomalies are defined as the time points when the data stream deviates from its
standard.
Besides this similarity, considering that these projects where designed to be
deployed in different network architectures, they differ in several aspects,
such as in the \gls*{qos} data collection, anomaly detection, and event
localization procedures.

\section{Argus}

In~\cite{argus_end_to_end_service_anomaly_detection_and_localization_from_an_isps_point_of_view}
is presented Argus, a system to
detect and localize problems in \gls*{isp}'s networks. To achieve this goal,
Argus uses network global information, and also data passively collected
from the \gls*{isp}'s viewpoint to infer
end-to-end \gls*{qos}, such as traffic to/from end-users to estimate achievable
download
speed~\cite{speed_testing_without_speed_tests_estimating_achievable_download_speed_from_passive_measurements}.

The system's analytics pipeline is illustrated in
Figure~\ref{fig:argus_pipeline}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/literature_review/argus_pipeline.png}
    \caption{Argus' pipeline.~\cite{argus_end_to_end_service_anomaly_detection_and_localization_from_an_isps_point_of_view}}
\label{fig:argus_pipeline}
\end{figure}%

The analysis starts with the Spatial Aggregation procedure, in which
end-users are clustered into user-groups.
Each user-group is characterized by a set of clients that share some
attributes, such as \gls*{as} or \gls*{bgp} prefix.
The used features define the possible fault locations to be inferred.
Also, this step improves the system's
scalability, since avoids keeping track the performance of all individual
end-users.
An example of a spatial aggregation is depicted in
Figure~\ref{fig:argus_spatial_aggregation}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./figures/literature_review/argus_spatial_aggregation.png}
    \caption{Argus' spatial aggregation.~\cite{argus_end_to_end_service_anomaly_detection_and_localization_from_an_isps_point_of_view}}
\label{fig:argus_spatial_aggregation}
\end{figure}%

The Temporal Aggregation phase determines how data
from different clients of an end-group are combined.
For each user-group, the
measurements samples of all end-users are grouped in time-bins, and for each
time-bin, a statistic, such as median or mean, is selected.
Each type of fault can be better tracked by a specific statistic.
As an example, the
minimum of the \glspl*{rtt} can capture the physical propagation delay,
while the average can be related with network congestion. Argus uses median
by default, since it was empirically verified as an
effective transformation to track network flaws, and also robust to individual
end-users variability caused by their local infrastructure.

The Event Detection procedure identifies anomalies in the
time series obtained in the Temporal Aggregation mechanism.
Argus uses a Holt-Winters~\cite{holt-winters_forecasting_some_practical_issues}
variation.

Using spatial and events' times correlations, the Event Localization step
infer fault locations.
However, the detailed description of how this
process is implemented was not published.

Finally, the detected problems are sorted according with
their significance, which considers metrics obtained
through the event detection
algorithm, and also the number of affected customers.

Argus was evaluated using \gls*{rtt} measurements of a \gls*{cdn} hosted in a
tier-1 \gls*{isp}\@.
During one month, 2909 anomalous events were detected.
In general, lower level user-groups were more responsible
for the events than the higher level groups,
and only a small fraction of the customers caused the user-groups anomalies.
Also, 90\% of the events lasted for
at most 1 hour, which was the used time-bin granularity.

Although not investigated by the Argus's authors, the fact that only a
small number
of clients are responsible for the user-groups events, is an indication that
fault localization can achieve higher precision
with finer spatial aggregation granularity. Besides, the system's accuracy was
not studied.

\section{NetNorad}

NetNorad~\cite{netnorad} consists of a Facebook's internal project to
automate its network's faults analysis.
Previous deployed techniques by Facebook exhibit several
disadvantages, for instance,
human-driven investigation may take hours. Also, cases known as gray failures,
can't be detected only collecting devices information through \gls*{snmp},
or command line interface.
For example, some equipments can't report its own malfunctioning, or
some problems can be related with the global network structure.

Facebook's network is structured hierarchically. At the
lowest level there are servers in racks, which are then organized in
clusters. A set of clusters in the same building, and attached to
the same network, characterize a data center. Data centers are grouped
through a network that interconnects them within the same region, and appends
them to the Facebook's global backbone. Figure
~\ref{fig:netnorad_network_architecture} illustrates this architecture.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/literature_review/netnorad_network_architecture.jpg}
    \caption{Facebook's network architecture.~\cite{netnorad}}
\label{fig:netnorad_network_architecture}
\end{figure}%

Unlike Argus, NetNorad uses active probing to assess loss and \gls*{rtt}
statistics.  Facebook's servers ping each other, in which
a pinger sends \gls*{udp} packets to responders, and the latter
sends the packets back. The process happens in turns. Each pinger
sends packets to all targets, collects the responses, and then repeats
the procedure. A small number of pingers are placed in each cluster,
and the responders are
deployed on all machines. All pingers share a target list, which includes
at least two machines of every rack.

As with Argus, NetNorad applies spatial aggregation techniques.
The pingers group the responses of machines that belong to the same cluster,
and tags them according with their relative location.
Tags are defined by the following patterns:
``DC'' if the target cluster is in the pinger's data center;
``Region'' if the target cluster is outside the pinger's
data center but within the same region;
``Global'' if the target cluster is outside the pinger's
region.

With the tagging process, each cluster has three time series reflecting
different spatial viewpoints.
To mitigate problems, these data streams are tracked through
distinct percentiles over 10 minutes intervals.
For instance, a packet loss spike at the
50th percentile means that probably there is a failure affecting the majority of
machines, while a peak at the 90th and not at 50th
percentile can indicate a small fraction of anomalous servers.
For each combination of proximity tag and percentile, it is defined two
thresholds, one to trigger and another to clear an alarm.

Considering the three tags, if a high loss is detected in a cluster,
then the fault is probably located at the cluster's data center.
Also, if all clusters in a data center identify a \gls*{qos} degradation,
then the fault is likely to be placed a layer above the clusters.
Although these simple inferences can reduce the set of possible fault locations,
they are unable to exactly isolate them.
However, a Facebook tool called fbtracert
can improve this analysis, exploring multiple
paths between two endpoints, and checking the
packet loss levels at every hop. Nonetheless, fbtracert exhibits several
limitations.

When automatic investigation is unable to find the failure, then there
is a human involvement to find it. A detailed accuracy analysis is not
presented, however, the infrastructure allows alarms to be raised about 30
seconds far from the network events.

\section{CEM}

In~\cite{crowdsourcing_service_level_network_event_monitoring} is proposed a
framework called \gls*{cem}, in which a
monitoring software, that runs inside or alongside applications, is placed at
end-hosts, enabling the detection of service level events within seconds or
minutes.

In \gls*{cem}, each end-host passively collects performance metrics related with
a specific service, such as a \gls*{vod} application.
To increase the system's scalability,
each end-host identifies local problems by their own,
and pushes these information to a distributed storage to further analysis.
The framework doesn't specify how events should be detected,
however, they must be associated with service level problems.

To spatially isolate network flaws, locally detected events, and spatial
information, are centrally correlated.
The first subproblem of this step is to check if concurrent events
of different end-users are caused by a
network fault. There are several reasons to different hosts identify
simultaneous events not caused by the network.
For instance, a high volume of requests in a web service can impact the
end-hosts' service performance. Also, it is
possible that simultaneous events occur only by chance, as an example, users
can suffer signal interference on distinct wireless routers.
Therefore, through service specific dependencies, and the empirical rate of
simultaneous
events, \gls*{cem} provides a statistical model to determine if
concurrent problems are a coincidence.
In this model, the confidence of a network fault increases with the
number of hosts that detect the event, and also with the number of affected
metrics.
The detailed method indicating how to realize spatial and temporal
correlations to
localize problems is not specified.

\gls*{cem} was deployed and evaluated in a P2P system, using traces
collected
from users of the Ono plugin in the Vuze BitTorrent client. The system's
output was contrasted with \glspl*{isp}' public available reports. In general,
\gls*{cem} provides a high level system abstraction, lacking several
important deployment issues.
