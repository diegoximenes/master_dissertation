\chapter{Literature Review of Change Point Detection} 

- Here the change point problem will be "defined", including offline and online versions.
- we deal with univariate unevenly time series. Explain that some methods can be expanded no multivariate
- unknow number of change points
- we disconsider changes in periodicity

\section{Notation}

In this work an univariate time series composed of $n$ points is defined by two vectors: $\mathbf{x} = (x_{1}, ..., x_{n})$ and $\mathbf{y} = (y_{1}, ..., y_{n})$. The value $y_{i}$ indicates the $i-$th sampled value and $x_{i}$ indicates the associated sample time. It is assumed that the points are sorted by time, that is, $x_{i - 1} < x_{i}$ for $i = 1, ..., n$. Since we consider unevenly time series $x_{i} - x_{i - 1}$ can be different for different $i$ values. For $s \ge t$ the following convention is adopted $\mathbf{y}_{s:t} = (y_{s}, ..., y_{t})$.

The presence of $k$ change points indicates that the data is splitted into $k+1$ segments, also called windows. Let $\tau_{i}$ indicate the $i-$th change point for $i=1,...,k$. Let $\tau_{0} = 0$ and $\tau_{k + 1} = n$. Then, the $i-$th segment is defined by $\mathbf{y}_{\tau_{i - 1} + 1 : \tau_{i}}$, assuming that $\tau_{i - 1} < \tau_{i}$ for $i = 0, ..., k + 1$. Therefore $\boldsymbol \tau = (\tau_{0}, ..., \tau_{k + 1})$.

\section{Sliding Window Techniques}

Mainly a change point detection algorithm aim to find $k$ and $\boldsymbol \tau$.

Describe sliding window techniques in change point detection. Describe how two windows can be compared. Probably this technique will not be part of my solution, but I will write about it since is the simplest and most intuitive solution.

\section{Combinatorial Optimization Model}  

\subsection{Constrained Case}

Given a fixed value of $k$, one approach is to define a cost function that measure the homogeneity of a segment and therefore choose the change points that globally optimize this homogeneity. Let the cost of the $i$-th segment be defined as $C(y_{\tau_{i - 1} + 1 : \tau_{i}})$. The cost of a segmentation is then $\sum \limits_{i = 1}^{k + 1} C(y_{\tau_{i - 1} + 1 : \tau_{i}})$.

A commom choice for function $C$ is the MSE (Mean Squared Error) which can capture changes in the mean. Another usual approach is to consider a distributin model and use the negative maximum log-likelihood. The latter can capture changes in mean and variance, also considers that data within a segment is iid. Therefore, given a fixed $k$, the optimal segmentation is obtained through the following optimization problem: 

\begin{equation}
    F_{k, n} = \min_{\boldsymbol \tau_{1 : k}} \sum \limits_{i = 1}^{k + 1} C(y_{\tau_{i - 1} + 1 : \tau_{i}})
\end{equation}

This problem can be solved using dynamic programming:

\begin{equation}
    F_{k, t} = 
    \begin{cases}
        0, & \text{if } k = 0 \text{ and } t = 0 \\
        \infty, & \text{if } k = 0 \text{ and } t > 0 \\
        \displaystyle \min_{s \in \{0, ..., t - 1\}} \left[ F_{k - 1, s} + C(y_{s + 1 : t}) \right], & \text{ otherwise}
    \end{cases}
\end{equation}

Given that function $C$ can be evaluated in $O(1)$, the overal time complexity of this algorithm is $O(k n^2)$.

\subsection{Segment Cost Function Computation}

Several segment cost functions can be evaluated in $O(1)$ after a preprocessing strategy. Next is provided the procedures to achieve this efficiency using MSE, negative log likelihood of normal and exponential distributions.

\subsubsection{MSE}

\begin{equation}
    \mu_{s, t} = \frac{\sum \limits_{i = s}^{t} y_{i}}{t - s + 1}
\end{equation}

\begin{equation}
    \begin{aligned}
        MSE(\mathbf{y}_{s : t}) = \frac{\sum \limits_{i = s}^{t} (y_{i} - \mu_{s, t})^{2}}{t - s + 1} \\
        = \frac{\sum \limits_{i = s}^{t} (x_{i}^{2} - 2 x_{i} \mu_{s, t} + \mu_{s, t}^{2})}{t - s + 1} \\
        = \frac{\sum \limits_{i = s}^{t} x_{i}^{2} - 2 \mu_{s, t} \sum \limits_{i = s}^{t} x_{i} + (t - s + 1) \mu_{s, t}^{2}}{t - s + 1} \\
    \end{aligned}
\end{equation}

\begin{equation}
    S_{i} = 
    \begin{cases}
        0, & \text{if } i = 0 \\
        S_{i - 1} + y_{i}, & \text{otherwise}
    \end{cases}
\end{equation}

\begin{equation}
    Q_{i} = 
    \begin{cases}
        0, & \text{if } i = 0 \\
        Q_{i - 1} + y_{i}^{2}, & \text{otherwise}
    \end{cases}
\end{equation}

\begin{equation}
    \mu_{s, t} = \frac{S_{t} - S_{s - 1}}{t - s + 1}
\end{equation}

\begin{equation}
    \sum \limits_{i = s}^{t} x_{i}^{2} = Q_{t} - Q_{s - 1}
\end{equation}

\begin{equation}
    \sum \limits_{i = s}^{t} x_{i} = S_{t} - S_{s - 1}
\end{equation}

\subsubsection{Negative Log-Likelihood of Normal Distribution}
\subsubsection{Negative Log-Likelihood of Exponential Distribution}

- degenerate distribution

\subsection{Penalized Case}

When the number of change points is unknown.

\subsection{Pruning}
- heuristics

\section{Bayesian Inference}
I will erase this section if I don't use bayesian inference. Describe Fearnheard (offline) and MacKay (online) solutions. Say that there are other versions.

\section{HMM}
I will not describe HMM algorithms (viterbi, baum welch, etc), I will only describe how HMM have been used in change point detection. Describe Left-Right HMM, full HMM, and Regularized HMM in this problem.

\section{Other Algorithms}
Only cite other used algorithms and say why I chose the previous one to analyse.

\section{Performance Evaluation}
  Describe how datasets are constructed in literature. Describe how an algorithm output is evaluated.

