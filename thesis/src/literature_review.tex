\chapter{Literature Review of Change Point Detection} 

- Here the change point problem will be "defined", including offline and online versions.
- we deal with univariate unevenly time series. Explain that some methods can be expanded no multivariate
- unknow number of change points
- segments and time series with different length
- we disconsider changes in periodicity

\section{Notation}

In this work an univariate time series composed of $n$ points is defined by two vectors: $\mathbf{x} = (x_{1}, ..., x_{n})$ and $\mathbf{y} = (y_{1}, ..., y_{n})$. The value $y_{i}$ indicates the $i-$th sampled value and $x_{i}$ indicates the associated sample time. It is assumed that the points are sorted by time, that is, $x_{i - 1} < x_{i}$ for $i = 1, ..., n$. Since we consider unevenly time series $x_{i} - x_{i - 1}$ can be different for different $i$ values. For $s \ge t$ the following convention is adopted $\mathbf{y}_{s:t} = (y_{s}, ..., y_{t})$.

The presence of $k$ change points indicates that the data is splitted into $k+1$ segments, also called windows. Let $\tau_{i}$ indicate the $i-$th change point for $i=1,...,k$. Let $\tau_{0} = 0$ and $\tau_{k + 1} = n$. Then, the $i-$th segment is defined by $\mathbf{y}_{\tau_{i - 1} + 1 : \tau_{i}}$, assuming that $\tau_{i - 1} < \tau_{i}$ for $i = 0, ..., k + 1$. Therefore $\boldsymbol \tau = (\tau_{0}, ..., \tau_{k + 1})$.

\section{Sliding Window Techniques}

Mainly a change point detection algorithm aim to find $k$ and $\boldsymbol \tau$.

Describe sliding window techniques in change point detection. Describe how two windows can be compared. Probably this technique will not be part of my solution, but I will write about it since is the simplest and most intuitive solution.

\section{Optimization Model}  

\subsection{Constrained Case}

Given a fixed value of $k$, one approach is to define a cost function that measure the homogeneity of a segment and therefore choose the change points that globally optimize this homogeneity. Let the cost of the $i$-th segment be defined as $C(y_{\tau_{i - 1} + 1 : \tau_{i}})$. The cost of a segmentation is then $\sum \limits_{i = 1}^{k + 1} C(y_{\tau_{i - 1} + 1 : \tau_{i}})$.

A commom choice for function $C$ is the MSE (Mean Squared Error) which can capture changes in the mean. Another usual approach is to consider a distributin model and use the negative maximum log-likelihood. The latter can capture changes in mean and variance, also considers that data within a segment is iid. Therefore, given a fixed $k$, the optimal segmentation is obtained through the following optimization problem: 

\begin{equation}
    F_{k, n} = \min_{\boldsymbol \tau_{1 : k}} \sum \limits_{i = 1}^{k + 1} C(y_{\tau_{i - 1} + 1 : \tau_{i}})
\end{equation}

This problem can be solved using dynamic programming:

\begin{equation}
    F_{k, t} = 
    \begin{cases}
        0, & \text{if } k = 0 \text{ and } t = 0 \\
        \infty, & \text{if } k = 0 \text{ and } t > 0 \\
        \displaystyle \min_{s \in \{0, ..., t - 1\}} \left[ F_{k - 1, s} + C(y_{s + 1 : t}) \right], & \text{ otherwise}
    \end{cases}
\end{equation}

The overal time complexity of this algorithm is $O(k n^2 f(n))$, where $f(n)$ is the time complexity to evaluate $C$.

The formulation can consider a minimum value of a segment

\subsection{Segment Cost Function}

Several segment cost functions can be evaluated in $O(1)$ after a preprocessing phase, implying in an overall $O(k n^2)$ time complexity. Next is provided the procedures to achieve this efficiency using MSE, negative maximum log-likelihood of normal and exponential distributions.

\subsubsection{MSE}

Let $\mu_{s, t}$ the mean value of the segment $\mathbf{y}_{s : t}$:

\begin{equation}
    \mu_{s, t} = \frac{\sum \limits_{i = s}^{t} y_{i}}{t - s + 1}
\end{equation}

Then, the $MSE(\mathbf{y}_{s : t})$ is defined as:

\begin{equation}
    \label{mse}
    \begin{aligned}
        MSE(\mathbf{y}_{s : t}) = \frac{\sum \limits_{i = s}^{t} (y_{i} - \mu_{s, t})^{2}}{t - s + 1} \\
        = \frac{\sum \limits_{i = s}^{t} (y_{i}^{2} - 2 y_{i} \mu_{s, t} + \mu_{s, t}^{2})}{t - s + 1} \\
        = \frac{\sum \limits_{i = s}^{t} y_{i}^{2} - 2 \mu_{s, t} \sum \limits_{i = s}^{t} y_{i} + (t - s + 1) \mu_{s, t}^{2}}{t - s + 1} \\
    \end{aligned}
\end{equation}

Let $S_{i} = \sum \limits_{j = 0}^{i} y_{j}$, that is, $\mathbf{S}$ represents the prefix sum of $\mathbf{y}$ for different indexes. $\mathbf{S}$ can be computed in $O(n)$ with the following dynamic programming procedure: 

\begin{equation}
    S_{i} = 
    \begin{cases}
        0, & \text{if } i = 0 \\
        S_{i - 1} + y_{i}, & \text{otherwise}
    \end{cases}
\end{equation}

Let $Q_{i} = \sum \limits_{j = 0}^{i} y_{j}^{2}$. As with $\mathbf{S}$, $\mathbf{Q}$ can be computed in $O(n)$: 

\begin{equation}
    Q_{i} = 
    \begin{cases}
        0, & \text{if } i = 0 \\
        Q_{i - 1} + y_{i}^{2}, & \text{otherwise}
    \end{cases}
\end{equation}

Given that $\mathbf{S}$ and $\mathbf{Q}$ are previously computed, it is possible to evaluate the following equations in $O(1)$: 

\begin{equation}
    \label{mean_o1}
    \mu_{s, t} = \frac{S_{t} - S_{s - 1}}{t - s + 1}
\end{equation}

\begin{equation}
    \label{prefixsum1_o1}
    \sum \limits_{i = s}^{t} y_{i} = S_{t} - S_{s - 1}
\end{equation}

\begin{equation}
    \label{prefixsum2_o1}
    \sum \limits_{i = s}^{t} y_{i}^{2} = Q_{t} - Q_{s - 1}
\end{equation}

With equations \ref{mean_o1}, \ref{prefixsum1_o1}, \ref{prefixsum2_o1} it is possible to observe that \ref{mse} can be computed in $O(1)$ given an $O(n)$ precomputation.  

\subsubsection{Negative Log-Likelihood of Normal Distribution}

The pdf of a Gaussian distribution is given by:

\begin{equation}
    N(x | \mu, \sigma^{2}) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x - \mu)^{2}}{2 \sigma^{2}}}
\end{equation}

The maximum likelihood estimators for the segment $\mathbf{y}_{s : t}$ are:

\begin{equation}
    \widehat{\mu}_{s, t} = \frac{\sum \limits_{i = s}^{t} y_{i}}{t - s + 1} 
\end{equation}

\begin{equation}
    \widehat{\sigma}_{s, t}^{2} = \frac{\sum \limits_{i = s}^{t} (y_{i} - \widehat{\mu}_{s, t})^{2}}{t - s + 1} 
\end{equation}

Through these equations is possible to observe that negative of the maximum log likelihood can be expressed using $MSE$:

\begin{equation}
    \begin{aligned}
        NLL_{s, t} = \frac{(t - s + 1) \ln(2 \pi)}{2} + \frac{(t - s + 1) \ln(\widehat{\sigma}_{s, t}^{2})}{2} + \frac{\sum \limits_{i = s}^{t} (y_{i} - \widehat{\mu}_{s, t})^{2}}{2 \widehat{\sigma}_{s, t}^{2}} \\
        %= \frac{(t - s + 1) \ln(2 \pi)}{2} + \frac{(t - s + 1) \ln(MSE_{s, t})}{2} + \frac{t - s + 1}{2} 
        = \left( \frac{t - s + 1}{2} \right) [\ln(2 \pi) + \ln(MSE_{s, t}) + 1]
    \end{aligned}
\end{equation}

Therefore, the negative log likelihood can also be computed in $O(1)$ with an $O(n)$ precomputation.

\subsubsection{Negative Log-Likelihood of Exponential Distribution}

The pdf of an exponential distribution is given by:

\begin{equation}
    f(x) = 
    \begin{cases}
        \lambda e^{- \lambda x}, & \text{if } x \in [0, \infty) \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

The maximum likelihood estimator of the segment $\mathbf{y}_{s : t}$ is:

\begin{equation}
    \widehat{\lambda}_{s, t} = \frac{(t - s + 1)}{\sum \limits_{i = s}^{t} y_{i}}
\end{equation}

The negative log likelihood can be expressed by:

\begin{equation}
    \begin{aligned}
        NLL_{s, t} = - (t - s + 1) \ln(\widehat{\lambda}_{s, t}) + \widehat{\lambda}_{s, t} \sum \limits_{i = s}^{t} y_{i} \\
        = - (t - s + 1) \left[ \ln \left( \frac{t - s + 1}{S_{t} - S_{s - 1}} \right) + 1 \right] \\
    \end{aligned}
\end{equation}

Therefore the exponential distribution can also be used as a cost function with $O(1)$ evaluation and $O(n)$ precomputation.

Using continuous distributions as the cost function can led to practical dificulties. The first one is that segments can form degenerate distributions, that is, the points of a segment can have zero variance. In these cases the negative likelihood functions will not be defined. Two approaches to avoid this situation is 1) try to avoid degenerate segments adding a white noise to the time series. 2) consider a constant and small value to degenerate segments. Since a single point compose a degenerate distribution to handle this case the options are the xonstant value or doesn`t allow segments with length equal to one.

Since the likelihood of different continuous distributions can`t be directly compared, is not possible to apply the segmentation algorithm considering different types of continuos distributions. One possibility to handle this is to apply automatic methods to check which kind of distribution fits better to the segment, such as Kolmogorov-Smirnov. This was not approached in this work due to the computation time efficiency decrease and that since we deal with small number of data possibly these methods would have a poor performance.

However, when considering discrete distributions is possible to direct compare the likelihood of two different 

\subsubsection{Negative Log-Likelihood of Poisson Distribution}
\subsubsection{Negative Log-Likelihood of Binomial Distribution}

\subsection{Penalized Case}

When the number of change points is unknown a common approach is to solve the constrained case for different values of k and penalize this number of change points, the bigger k biggest is the penalty.

Let $g(k)$ be the penalty function. Then the new optimization problem is:

\begin{equation}
    \min_{k} F_{k, n} + g(k)
\end{equation}

An approach to solve this problem is to solve the constrained for $k = 0, ..., K$. This lead to an $O(K^{2} n^{2} f(n))$

However if the penalty function is linear in $k$ the problem can be formulated in a more efficient way.

Let the penalty function be in the form:

\begin{equation}
    g(k) = (\alpha + 1) k
\end{equation}

Therefore the optimization problem can be formulated as:

\begin{equation}
    \begin{aligned}
        G_{n} = \min_{k, \boldsymbol \tau_{1 : k}} \left[ \sum \limits_{i = 1}^{k + 1} C(y_{\tau_{i - 1} + 1 : \tau_{i}}) + (\alpha + 1) k \right] \\
        = \min_{k, \boldsymbol \tau_{1 : k}} \sum \limits_{i = 1}^{k + 1} \left[ C(y_{\tau_{i - 1} + 1 : \tau_{i}}) + \alpha \right] \\
    \end{aligned}
\end{equation}

This problem can be solved using dynamic programming:

\begin{equation}
    G_{t} = 
    \begin{cases}
        0, & \text{if } t = 0 \\
        \displaystyle \min_{s \in \{0, ..., t - 1\}} \left[ G_{s} + C(y_{s + 1 : t}) + \alpha \right], & \text{ otherwise}
    \end{cases}
\end{equation}

\subsection{Pruning}

\section{Bayesian Inference}
I will erase this section if I don't use bayesian inference. Describe Fearnheard (offline) and MacKay (online) solutions. Say that there are other versions.

\section{HMM}
I will not describe HMM algorithms (viterbi, baum welch, etc), I will only describe how HMM have been used in change point detection. Describe Left-Right HMM, full HMM, and Regularized HMM in this problem.

\section{Other Algorithms}
Only cite other used algorithms and say why I chose the previous one to analyse.

- Binary Segmentation

\section{Performance Evaluation}
  Describe how datasets are constructed in literature. Describe how an algorithm output is evaluated.

